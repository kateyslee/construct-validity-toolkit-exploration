{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0839b022",
   "metadata": {},
   "source": [
    "# Import and Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ecff1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb1b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['summary', 'expert_annotations', 'turker_annotations', 'references',\n",
      "       'model_id', 'raw', 'mistral_relevance', 'mistral_fluency',\n",
      "       'mistral_coherence', 'mistral_consistency'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith(\"notebooks\"):\n",
    "    prefix = \"../\"\n",
    "else:\n",
    "    prefix = \"./\"\n",
    "\n",
    "base_dir = os.path.join(prefix, \"summeval-data\")\n",
    "\n",
    "df = pd.read_csv(os.path.join(base_dir, \"summeval_with_mistral_ratings.csv\"))\n",
    "# df = pd.read_csv(os.path.join(base_dir, \"summeval_og.csv\"))\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78c0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8328d",
   "metadata": {},
   "source": [
    "# Processing SummEval Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786f3a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'coherence': 2, 'consistency': 1, 'fluency': 4, 'relevance': 2}, {'coherence': 1, 'consistency': 1, 'fluency': 2, 'relevance': 1}, {'coherence': 1, 'consistency': 1, 'fluency': 3, 'relevance': 2}, {'coherence': 3, 'consistency': 3, 'fluency': 4, 'relevance': 3}, {'coherence': 3, 'consistency': 3, 'fluency': 4, 'relevance': 3}, {'coherence': 3, 'consistency': 3, 'fluency': 4, 'relevance': 3}, {'coherence': 3, 'consistency': 3, 'fluency': 4, 'relevance': 3}, {'coherence': 3, 'consistency': 3, 'fluency': 4, 'relevance': 3}]\n"
     ]
    }
   ],
   "source": [
    "# Generate all_annotations column\n",
    "# 3 experts, 5 turker (crowdsources)\n",
    "\n",
    "df[\"expert_annotations\"] = df[\"expert_annotations\"].apply(ast.literal_eval)\n",
    "df[\"turker_annotations\"] = df[\"turker_annotations\"].apply(ast.literal_eval)\n",
    "df[\"all_annotations\"] = df[\"expert_annotations\"] + df[\"turker_annotations\"]\n",
    "\n",
    "print(df[\"all_annotations\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All human ratings are range 1~5, disc column to discretize to {0, 1}\n",
    "DISC_THRESHOLD = 3\n",
    "\n",
    "# New columns: \n",
    "# Scores by property: scores_{prop}_{expert/turker/all}\n",
    "# Variances: var_{prop}_{expert/turker/all}\n",
    "# Means: mean_{prop}_{expert/turker/all}\n",
    "# Difference of Mistral (LLM) score and mean_{prop}_all : diff_{prop}\n",
    "# NOTE: Mistral score is a preliminary score generated (1-5) prior to the proper heuristics.ipynb run across 5 judges\n",
    "\n",
    "for prop in properties:\n",
    "    df[f\"scores_{prop}_expert\"] = pd.Series([[]]*len(df), dtype=object)\n",
    "    df[f\"scores_{prop}_turker\"] = pd.Series([[]]*len(df), dtype=object)\n",
    "    df[f\"scores_{prop}_all\"] = pd.Series([[]]*len(df), dtype=object)\n",
    "\n",
    "    df[f\"var_{prop}_expert\"] = np.nan\n",
    "    df[f\"var_{prop}_turker\"] = np.nan\n",
    "    df[f\"var_{prop}_all\"] = np.nan\n",
    "\n",
    "    df[f\"mean_{prop}_expert\"] = np.nan\n",
    "    df[f\"mean_{prop}_turker\"] = np.nan\n",
    "    df[f\"mean_{prop}_all\"] = np.nan\n",
    "\n",
    "    df[f\"var_{prop}_expert_disc\"] = np.nan\n",
    "    df[f\"var_{prop}_turker_disc\"] = np.nan\n",
    "    df[f\"var_{prop}_all_disc\"] = np.nan\n",
    "    \n",
    "    df[f\"diff_{prop}\"] = np.nan  # <-- new column for difference\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    expert_ann = row[\"expert_annotations\"]\n",
    "    turker_ann = row[\"turker_annotations\"]\n",
    "    all_ann = row[\"all_annotations\"]\n",
    "\n",
    "    for prop in properties:\n",
    "        expert_scores = [ann[prop] for ann in expert_ann if prop in ann]\n",
    "        turker_scores = [ann[prop] for ann in turker_ann if prop in ann]\n",
    "        all_scores = [ann[prop] for ann in all_ann if prop in ann]\n",
    "        expert_scores_disc = [ann[prop] >= DISC_THRESHOLD for ann in expert_ann if prop in ann]\n",
    "        turker_scores_disc = [ann[prop] >= DISC_THRESHOLD for ann in turker_ann if prop in ann]\n",
    "        all_scores_disc = [ann[prop] >= DISC_THRESHOLD for ann in all_ann if prop in ann]\n",
    "\n",
    "        df.at[idx, f\"scores_{prop}_expert\"] = expert_scores\n",
    "        df.at[idx, f\"scores_{prop}_turker\"] = turker_scores\n",
    "        df.at[idx, f\"scores_{prop}_all\"] = all_scores\n",
    "        \n",
    "        if expert_scores:\n",
    "            df.at[idx, f\"mean_{prop}_expert\"] = np.mean(expert_scores)\n",
    "        if turker_scores:\n",
    "            df.at[idx, f\"mean_{prop}_turker\"] = np.mean(turker_scores)\n",
    "        if all_scores:\n",
    "            mean_all = np.mean(all_scores)\n",
    "            df.at[idx, f\"mean_{prop}_all\"] = mean_all\n",
    "\n",
    "            if pd.notna(row.get(f\"mistral_{prop}\", np.nan)):\n",
    "                df.at[idx, f\"diff_{prop}\"] = mean_all - row[f\"mistral_{prop}\"]\n",
    "\n",
    "        if len(expert_scores) > 1:\n",
    "            df.at[idx, f\"var_{prop}_expert\"] = np.var(expert_scores, ddof=1)\n",
    "            df.at[idx, f\"var_{prop}_expert_disc\"] = np.var(expert_scores_disc, ddof=1)\n",
    "        if len(turker_scores) > 1:\n",
    "            df.at[idx, f\"var_{prop}_turker\"] = np.var(turker_scores, ddof=1)\n",
    "            df.at[idx, f\"var_{prop}_turker_disc\"] = np.var(turker_scores_disc, ddof=1)\n",
    "        if len(all_scores) > 1:\n",
    "            df.at[idx, f\"var_{prop}_all\"] = np.var(all_scores, ddof=1)\n",
    "            df.at[idx, f\"var_{prop}_all_disc\"] = np.var(all_scores_disc, ddof=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36b6c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['summary', 'expert_annotations', 'turker_annotations', 'references',\n",
      "       'model_id', 'raw', 'mistral_relevance', 'mistral_fluency',\n",
      "       'mistral_coherence', 'mistral_consistency', 'all_annotations',\n",
      "       'scores_coherence_expert', 'scores_coherence_turker',\n",
      "       'scores_coherence_all', 'var_coherence_expert', 'var_coherence_turker',\n",
      "       'var_coherence_all', 'mean_coherence_expert', 'mean_coherence_turker',\n",
      "       'mean_coherence_all', 'var_coherence_expert_disc',\n",
      "       'var_coherence_turker_disc', 'var_coherence_all_disc', 'diff_coherence',\n",
      "       'scores_consistency_expert', 'scores_consistency_turker',\n",
      "       'scores_consistency_all', 'var_consistency_expert',\n",
      "       'var_consistency_turker', 'var_consistency_all',\n",
      "       'mean_consistency_expert', 'mean_consistency_turker',\n",
      "       'mean_consistency_all', 'var_consistency_expert_disc',\n",
      "       'var_consistency_turker_disc', 'var_consistency_all_disc',\n",
      "       'diff_consistency', 'scores_fluency_expert', 'scores_fluency_turker',\n",
      "       'scores_fluency_all', 'var_fluency_expert', 'var_fluency_turker',\n",
      "       'var_fluency_all', 'mean_fluency_expert', 'mean_fluency_turker',\n",
      "       'mean_fluency_all', 'var_fluency_expert_disc',\n",
      "       'var_fluency_turker_disc', 'var_fluency_all_disc', 'diff_fluency',\n",
      "       'scores_relevance_expert', 'scores_relevance_turker',\n",
      "       'scores_relevance_all', 'var_relevance_expert', 'var_relevance_turker',\n",
      "       'var_relevance_all', 'mean_relevance_expert', 'mean_relevance_turker',\n",
      "       'mean_relevance_all', 'var_relevance_expert_disc',\n",
      "       'var_relevance_turker_disc', 'var_relevance_all_disc',\n",
      "       'diff_relevance'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3efcdd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary                      paul merson was brought on with only seven min...\n",
      "expert_annotations           [{'coherence': 2, 'consistency': 1, 'fluency':...\n",
      "turker_annotations           [{'coherence': 3, 'consistency': 3, 'fluency':...\n",
      "references                   [\"Andros Townsend an 83rd minute sub in Totten...\n",
      "model_id                                                                   M11\n",
      "                                                   ...                        \n",
      "mean_relevance_all                                                         2.5\n",
      "var_relevance_expert_disc                                                  0.0\n",
      "var_relevance_turker_disc                                                  0.0\n",
      "var_relevance_all_disc                                                0.267857\n",
      "diff_relevance                                                             0.5\n",
      "Name: 0, Length: 63, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81bc88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(os.path.join(base_dir, \"summeval_processed_full.jsonl\"), orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fe014",
   "metadata": {},
   "source": [
    "# Split df into Train, Val, Holdout Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23e9e1",
   "metadata": {},
   "source": [
    "Split sets used in HypotheSAEs code, not heuristics code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4139c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1120, Validation: 240, Holdout: 240\n"
     ]
    }
   ],
   "source": [
    "# Train 0.7, Val 0.15, Holdout 0.15\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "val_df, holdout_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Holdout: {len(holdout_df)}\")\n",
    "\n",
    "train_df.to_json(os.path.join(base_dir, \"summeval_processed_train.jsonl\"), orient=\"records\", lines=True)\n",
    "val_df.to_json(os.path.join(base_dir, \"summeval_processed_val.jsonl\"), orient=\"records\", lines=True)\n",
    "holdout_df.to_json(os.path.join(base_dir, \"summeval_processed_holdout.jsonl\"), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2218ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypothesaes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
